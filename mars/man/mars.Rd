% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mars.R
\name{mars}
\alias{mars}
\title{Multivariate Adaptive Regression Splines (MARS)}
\usage{
mars(formula, data, control)
}
\arguments{
\item{formula}{an object of class "formula":
a symbolic description of the model to be fitted.}

\item{data}{a data frame containing the variables in the model.}

\item{control}{an object of class "mars.control".}
}
\value{
an object of class "mars". It is a list containing call made to the mars function, formula argument, a vector of response variable, a matrix B, a list
Bfuncs (containing final basis functions), and x_names (containing name of explanatory variables), along with other outputs you get from an object of class "lm".
}
\description{
This package is used for flexible regression modelling of high dimensional data.
The resulting model is a product of spline basis functions, where the number of basis functions
as well as the parameters associated with each one (product degree and knot locations) are automatically
determined by the data. These basis functions and their components will be presented in the Bfuncs list, which is
a part of the output from "mars" function.
}
\details{
The formula argument takes in a formula same as an "lm" function. A typical formula has the form response ~ terms,
where response is the numeric response vector and terms is a series of terms which specifies a linear predictor for response.
The data argument is a data frame that consists of one column for response variable and one or more columns for explanatory variables.
The control argument takes in an object of class "mars.control". This can be created using function "mars.control". Mars.control function takes
in three arguments: Mmax (maximum number of basis functions you want to allow; default value is 2), d (used in Generalized Cross Validation criterion;
default value is 3), trace (if user wants to see what is happening as the mars function is fitting the data; default value is FALSE).
In the mars function, the whole algorithm can be divided into two sub-algorithms, forward step-wise and backward step-wise algorithm.
Forward step-wise algorithm is based on recursive partitioning, with the aim of optimally splitting the covariate space into the maximum number of sub-regions
specified by user in control argument. This algorithm is implemented using four loops. The outermost loop iterates over 1 to half of the Mmax.
Here we are looping up to half of Mmax because each iteration of this outer loop will add pairs of basis functions, where each basis function represents a
sub-region. Within this first loop, based on what iteration number of first loop is going on, second for loop tries to find the optimum basis function to split on.
The second for loop iterates from 1 to twice the iteration number of first loop minus 1, since that is the maximum number of basis function in the current model based on iteration
number of first loop. The third loop selects the variable to split on within this basis function, and it has to be a variable which has not already been used in this basis function.
Lastly, the fourth loop selects the split point (knot) on the variable. The variable and the split point is selected by fitting a model at every iteration and computing a measure of
lack of fit using GCV criterion. An optimum variable and split point is the one with the smallest value for GCV criterion. To record all these basis functions, there is a list called
Bfuncs and matrix called B. These are updated as the model selects the optimum splits. At each iteration of the first loop, the algorithm adds two basis function in pairs that are identical
except that a different side of a mirrored hinge is used for each basis function. A hinge function is defined by a sign (+/-), variable, and a split-point. In this way, MARS is searching
over all combinations of existing basis functions, variables in them, and values of each variable. Due to this, forward step-wise algorithm is "greedy" and therefore must be followed by
backward step-wise algorithm. Backward step-wise algorithm removes terms one by one, selecting the least effective term at each step until it finds the best submodel. Model subsets are compared
using the Generalized Cross Validation (GCV) criterion. This is implemented using two for loops. First loop iterates over all the possible model sizes starting from Mmax + 1. At each iteration of
this outer loop we set variable keeping track of lack of fit to infinity. The inner loop iterates over all the model terms in the current model under outer loop, and try removing one basis function
at a time, fitting the model and calculating lack of fit value using GCV criterion. If lack of fit value observed is lowest in this iteration of the inner loop, it updates the index set of the current model, and if
this LOF value is also the lowest in all iterations so far of the outer loop, it updates the index set of the final model that will be returned as an output.
}
\examples{
mars(formula = y ~ ., data = marstestdata, control = testmc)
}
\references{
Friedman, J.H. (1991). Multivariate Adaptive Regression Splines. The Annals of Statistics, Vol. 19, No. 1 (Mar., 1991), pp. 1-67
}
\seealso{
summary.mars for summary of mars object, print.mars for printing mars object, plot.mars for plotting mars object, and predict.mars for prediction.
}
\author{
Samir Arora and Gurpal Singh Tulli
}
